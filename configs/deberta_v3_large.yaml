# CLARITY Training Config: DeBERTa-v3-large (A100 optimized)
# Observed: base ≈ 0.27–0.28 strict Task2 F1 on dev; large TBD
# Memory: ~12-14GB with bf16, batch_size=8, max_length=384

model_name: microsoft/deberta-v3-large
task: evasion
max_length: 384
precision: bf16           # auto-selects bf16 on A100 (use fp16 for T4/V100)
batch_size: 8
grad_accum: 2             # effective batch = 16
lr: 1.0e-5
weight_decay: 0.01
warmup_ratio: 0.06
epochs: 5
patience: 3
dropout: 0.1
seed: 42
output_dir: checkpoints/deberta_v3_large
use_focal_loss: false
use_class_weights: true   # critical for macro F1 with imbalanced labels
label_smoothing: 0.05
alpha: 0.7
consistency_beta: 0.1
fp16: false               # superseded by precision: bf16
