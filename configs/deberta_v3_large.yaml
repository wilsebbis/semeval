# CLARITY Training Config: DeBERTa-v3-large (A100 optimized)
# Observed: base ≈ 0.27–0.28 strict Task2 F1 on dev; large TBD
# Memory: ~12-14GB with bf16, batch_size=16, max_length=384

model_name: microsoft/deberta-v3-large
task: evasion
max_length: 384
precision: bf16           # fp32 weights + bf16 autocast on A100
batch_size: 16            # A100 80GB handles 16 at 384 easily
grad_accum: 1             # effective batch = 16
lr: 2.0e-5               # 2x base LR — large models need more signal
weight_decay: 0.01
warmup_ratio: 0.06
epochs: 5
patience: 3
dropout: 0.1
seed: 42
output_dir: checkpoints/deberta_v3_large
use_focal_loss: false
use_class_weights: false  # baseline first — recover base-level F1
label_smoothing: 0.05
alpha: 0.7
consistency_beta: 0.1
fp16: false               # superseded by precision: bf16
