# CLARITY: Unmasking Political Question Evasions â€” SemEval 2026

A competitive solution for [SemEval 2026 Task: CLARITY](https://konstantinosftw.github.io/CLARITY-SemEval-2026/), classifying political response clarity and evasion techniques from the ["I Never Said That" (Thomas et al., 2024)](https://aclanthology.org/2024.findings-emnlp.300/) taxonomy.

## Tasks

| Task | Classes | Labels | Metric |
|------|---------|--------|--------|
| **Task 1** â€” Clarity | 3-way | Clear Reply, Ambivalent Reply, Clear Non-Reply | Macro F1 |
| **Task 2** â€” Evasion | 9-way | Explicit, Implicit, General, Partial/half-answer, Dodging, Deflection, Declining to answer, Claims ignorance, Clarification | Macro F1 |

Task 1 is derived from Task 2 via a fixed taxonomy mapping â€” see [Architecture](docs/architecture.md).

## Quick Start

```bash
# 1. Install dependencies
uv sync                    # or: pip install -e ".[dev]"

# 2. Prepare data
git clone https://huggingface.co/datasets/ailsntua/QEvasion
bash scripts/prepare_data.sh

# 3. Train (DeBERTa-v3-base)
bash scripts/run_train_deberta.sh

# 4. Predict
bash scripts/run_predict.sh checkpoints/deberta_v3_base/best_model.pt data/test.csv

# 5. Run tests
pytest tests/ -v
```

## Model Configurations

| Config | Model | Task2 F1 | VRAM | Notes |
|--------|-------|----------|------|-------|
| [`roberta_base.yaml`](configs/roberta_base.yaml) | RoBERTa-base | ~0.45â€“0.52 | ~4GB | Fast baseline, MPS-compatible |
| [`deberta_v3_base.yaml`](configs/deberta_v3_base.yaml) | DeBERTa-v3-base | ~0.48â€“0.56 | ~6GB | Local workhorse |
| [`deberta_v3_large.yaml`](configs/deberta_v3_large.yaml) | DeBERTa-v3-large | ~0.56â€“0.65 | ~16GB | Competitive (CUDA recommended) |
| [`deberta_multitask.yaml`](configs/deberta_multitask.yaml) | DeBERTa-v3-base | ~0.48â€“0.55 | ~6GB | Joint evasion + clarity heads |

All configs use `label_smoothing: 0.05` and `use_class_weights: true`.

## Documentation

| Page | Contents |
|------|----------|
| ğŸ“ [Architecture](docs/architecture.md) | Model design, taxonomy hierarchy, loss functions |
| ğŸ“Š [Evaluation](docs/evaluation.md) | Multi-annotator scoring, metrics, early stopping |
| ğŸ‹ï¸ [Training Guide](docs/training.md) | Configs, CLI args, recipes, ensemble strategy |
| ğŸ [Apple Silicon](docs/apple_silicon.md) | MPS workarounds, CPU fallback, HF cache fixes |
| ğŸ“„ [Data Format](docs/data_format.md) | CSV schema, label taxonomy, class distributions |

## Project Structure

```
semeval/
â”œâ”€â”€ configs/                   # YAML training configurations
â”‚   â”œâ”€â”€ roberta_base.yaml
â”‚   â”œâ”€â”€ deberta_v3_base.yaml
â”‚   â”œâ”€â”€ deberta_v3_large.yaml
â”‚   â””â”€â”€ deberta_multitask.yaml
â”œâ”€â”€ docs/                      # Wiki sub-pages
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ evaluation.md
â”‚   â”œâ”€â”€ training.md
â”‚   â”œâ”€â”€ apple_silicon.md
â”‚   â””â”€â”€ data_format.md
â”œâ”€â”€ scripts/                   # Shell scripts
â”‚   â”œâ”€â”€ prepare_data.sh
â”‚   â”œâ”€â”€ run_train_roberta.sh
â”‚   â”œâ”€â”€ run_train_deberta.sh
â”‚   â”œâ”€â”€ run_train_deberta_large.sh
â”‚   â”œâ”€â”€ run_predict.sh
â”‚   â””â”€â”€ run_ensemble.sh
â”œâ”€â”€ src/clarity/               # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ labels.py              # Taxonomy, hierarchy, normalization
â”‚   â”œâ”€â”€ data.py                # Dataset loading, tokenization
â”‚   â”œâ”€â”€ models.py              # HierarchicalClassifier, MultitaskClassifier
â”‚   â”œâ”€â”€ train.py               # Training loop CLI
â”‚   â”œâ”€â”€ eval.py                # F1 scoring, multi-annotator logic
â”‚   â”œâ”€â”€ predict.py             # Inference + submission generation
â”‚   â””â”€â”€ utils.py               # Seeding, config, logging
â”œâ”€â”€ tests/                     # Test suite (43 tests)
â”‚   â”œâ”€â”€ test_label_mapping.py
â”‚   â””â”€â”€ test_eval_multiannotator.py
â”œâ”€â”€ data/                      # Generated by prepare_data.sh
â”œâ”€â”€ checkpoints/               # Saved models
â”œâ”€â”€ submissions/               # Output CSVs
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## Reproducibility

- Deterministic seeding via `seed_everything()` (default: 42)
- CUDA deterministic mode enabled
- YAML configs capture all hyperparameters
- Checkpoints include full args dict for perfect reconstruction

## Citation

```bibtex
@inproceedings{thomas-etal-2024-never,
    title = "``{I} Never Said That'': A dataset, taxonomy and baselines on response clarity classification",
    author = "Thomas, Konstantinos and Filandrianos, Giorgos and Lymperaiou, Maria and Zerva, Chrysoula and Stamou, Giorgos",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    year = "2024",
    url = "https://aclanthology.org/2024.findings-emnlp.300",
}
```
